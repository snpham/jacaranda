\documentclass[11pt,sigconf]{acmart}

\usepackage{booktabs} % For formal tables

\graphicspath{{figure/}{figures/}}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[Spring '22]{CSCI 6502 - Big Data Analytics}{March 2022}{Boulder , CO, USA} 
\acmYear{2022}
\copyrightyear{2022}

\acmPrice{15.00}


\begin{document}
\title{Galaxy Morphological Classification with Big Data: An Analysis of Large Spectral and Photometric Datasets}
\titlenote{Produces the permission block, and copyright information}
\subtitle{Extended Abstract}

\author{Son Pham}
\authornote{Note}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{CU Boulder}
  \city{Boulder} 
  \state{CO} 
}
\email{son.pham-2@colorado.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{S. Pham}


\begin{abstract}

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10002950.10003714</concept_id>
  <concept_desc>Mathematics of computing~Mathematical analysis</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10002951.10002952</concept_id>
  <concept_desc>Information systems~Data management systems</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010257</concept_id>
  <concept_desc>Computing methodologies~Machine learning</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10010147.10010919</concept_id>
  <concept_desc>Computing methodologies~Distributed computing methodologies</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Mathematics of computing~Mathematical analysis}
\ccsdesc[300]{Information systems~Data management systems}
\ccsdesc[300]{Computing methodologies~Machine learning}
\ccsdesc[300]{Computing methodologies~Distributed computing methodologies}
\ccsdesc[300]{Applied computing~Astronomy}

% We no longer use \terms command
%\terms{Theory}

\keywords{ACM proceedings}


\maketitle

\section{Introduction}

% Refer to \verb|acmart.pdf| \cite{veytsmanlatex} (\url{https://www.ctan.org/pkg/acmart}, 
% \url{http://www.acm.org/publications/proceedings-template}) for additional examples and instructions.
Big data analytics is transforming the way scientists and astronomers study the universe. 
With each large scale telescope on the ground or in space, dozens of gigabytes of data 
is being generated per day - an amount that will take an increasingly amount of time
to explore and process. For example, the Hubble Space Telescope generates about 120G of 
scientific data every week \cite{tillman}. Sky surveys conducted by telescopes such as
the Sloan Digital Sky Survey (SDSS) produces data releases each year that can be as high as
100's of TBs. SDSS provides a wide range of data types, such as optical spectra,
infrared spectra, and imaging. 
\\
Recent discoveries, such as the presence of over 100 black holes 
in the center of our Milky Way, was realized using data from decades ago generated by
the Chandra satellite. Scientific and technological advancement in combination led the 
way to this capability. With the increase in computational performance, astronomers 
now have the capabilities to explore these large datasets without the need to invest 
in large ground-based optical telescopes or work in a research lab. 
As big data analytics continue to grow, these discoveries will become more common as 
scientists continue to collect and process more of the data that is available. As technological
advancements grow, they will have more readily available tools and platforms for their 
analysis. With the cummilation telescopes and technologies present today, the entire 
electromagnetic spectrum can be observed within a patch of sky of interest. 
\\
SDSS datasets, such provide many different data types, provides the opportunity to
explore galaxies, stars, and quasars too distant to view for amateur astronomers and 
conduct many different types of analysis on them. The analytics of interest to the
authors include the use of optical imaging and spectral data. Legacy imaging generated 
from prior SDSS programs, if used with machine learning classification techniques, 
can provide automatic classification of morphological properties of these galaxies. 
Quatities measured from these images and spectra readings can also provide information
such as magnitudes, redshifts, and object classifications. In particular, redshift can
be a good indicator of galaxy morphology. 
\\
This paper will go over in detail the approach to perform morphological classification of 
galaxies using these available information from various sources. The main source of interest
for the author is SDSS's data, which includes 100's of terabytes of data covering more
than one-third of the entire celestial sphere \cite{SDSS}.


\section{Literature Survey}

\subsection{External Studies}

\subsection{Studies Based on This Dataset}




\section{Datasets}

SDSS Imaging dataset.
\\

\begin{table}[]
  \begin{tabular}{ |p{3cm}|p{2.8cm}|p{1.6cm}|  } \hline
   Parameter & Description & Unit  \\ \hline
   Total unique area covered & 14,555  & sq. deg \\ \hline
   Total area of imaging (including overlaps) & 31,637 & sq. deg \\ \hline
   Individual image field size & 1361x2048 (0.0337) & pixels (sq. deg) \\ \hline
   catalog objects & 1,231,051,050 & (-) \\ \hline
   unique detections & 932,891,133 & (-) \\ \hline
   Median PSF FWHM, r-band & 1.3 & arcsec \\ \hline
   Pixel scale & 0.396 & arcsec \\ \hline
  \end{tabular}
  \end{table}

\section{Approach}

\subsection{Data Cleaning and Pre-Processing}


\subsection{Data Integration}
There are many available sources that provide celestial sky survey data. In astronomy,
many use a common file transfer type called Flexible Image Transport System (FITS), 
mainly used for image file transfer, but can also be used to store any type of data 
that can fit in a multi-dimensional array or table. Typical sky surveys contain 
equitorial coordinates, defined relative to the celestial sphere, that can be used to 
identify position of an object in the sky. These surveys also contain object ID's that 
is used to reference a particular object within a catalog or database. These object 
ID's are then used as keys to integrate various databases together. 
\\
To link the many objects from various data sources, the common method is to obtain
the right ascension (RA) and declination (DEC) of the object in the celestial frame. 
RA is measured from the vernal equinox to the point of interest, going positive east
along the celestial equator. The vernal equinox is the intersection of the celestial 
equator and the ecliptic where the ecliptic crosses the ascending node. The RA varies 
from 0 to 24 hours, and typically measured in hours-minutes-seconds (HMS). To get RA 
in decimal degrees, Equation \ref{eqn_ra} is used. 
\\
DEC is the angle from the celestial equator to the point of interest, positive going 
north (and negative going south). DEC varies from -90 to +90 degrees, with fractions 
of a degree measured in arcminutes and arcseconds. The notation for DEC is 
degrees-minutes-seconds, with a full circle providing 360 degrees, each degree having 
60 arcminutes and each arcminute having 60 arcseconds. DEC in decimal degrees can be 
obtained using Equation \ref{eqn_dec}. Some surveys, such as the SuperCOSMOS survey, 
already has their data in this format, but sources such as the Australia Telescope 
20-GHz Survey contains coordinates in the standard form and must be converted.

\begin{equation} \label{eqn_ra}
  \begin{split}
    RA_{deg}= 15(hours + \frac{minutes}{60} + \frac{seconds}{3600}) 
  \end{split}
\end{equation}

\begin{equation} \label{eqn_dec}
  \begin{split}
  & DEC_{deg}= degrees+ \\
  & sign(degrees)(\frac{arcminutes}{60} + \frac{arcseconds}{3600})
  \end{split}
\end{equation}

Once all sources format is converted in to a common decimal degrees format, the 
datasets can be read and integrated into a common database. To connect objects 
from different surveys together, we must look at the coordinates of the
objects within each catalog and cross-match to find its closest counterpart, 
measured by the angular distance between them. Since the position of two given 
objects are measured as points on a sphere, the great-circle distance must be 
computed. This can be achieved using the haversine formula, which computes the 
distance of two points using a set of right ascension and declination angles, 
as shown in \ref{eqn_haversine}, where $\alpha$ and $\delta$ are the corresponding
RA and DEC of the two points.

\begin{equation} \label{eqn_haversine}
  \begin{split}
   & d_{\theta} = 2 \arcsin \cdot \\
   & {\sqrt{\sin^2\frac{\left| \delta_1 - \delta_2 \right|}{2} + \cos{\delta_1}\cos{\delta_2}\sin^2\frac{\left| \alpha_1 - \alpha_2 \right|}{2}}}  
  \end{split}
\end{equation}

Although this computation is useful in helping integrate multiple sources together, 
the algorithm does not scale well. Since the algorithm requires the calculation of
the distance of each data object from one catalog to each data object in the second
catalog, the time complexity is O(n*m), where n and m are the counts of data objects
in the catalogs. This is not an issue for catalogs with a few hundred samples,
but typical catalogs such as the SDSS contains over a million data points, which will
be prohibitively expensive to compute.

An improved algorithm was used for this project's analysis. The first method was to
use Python's numpy array structure to automatically loop through a catalog dataset. This
provided a speed boost compared to standard Python standard library, but slowed down
significantly when working with datasets greater than 10 million data objects. The second
method was to contrain the search to within a given angular radius away from the desired
celestial location, which will disregard points located outside of this radius. To find
this index value within a catalog, a binary search is performed, where the catalog is 
repeatedly split in half until the value of interest is found. This provided a boost in
performance for small and medium sized datasets, but slows down significantly for datasets 
with over 100 million data objects. The final method was to use a k-d tree to perform
the crossmatching. Similar to the binary search algorithm, the k-d tree divides the 
k-dimensional space into two parts recursively until each segment is it's own leaf. 








\subsection{Statistical Analysis}
One of the major issues in working with large datasets is that the standard analytical
evaluation of statistical formulas requires an entire batch of data samples to
be stored in memory for computation. Having 100's of thousands of samples can
quickly exceed the memory limit of an average laptop or computer. This is especially
the case when running analysis on embedded systems where memory is a driving constraint
of performance. Stream processing allows a user to run calculations as the data comes in
and releases memory that held samples of data from previous use.
\\
Welford's method for statistical analysis allows the use of stream processing to
compute some statistical parameters \cite{welford}. This method gives accurate estimates of the 
mean and variance without having to store all the data in memory. The standard process for computing
standard deviation is to compute the mean of the data in one pass, then calculate the square
deviation of values from the mean in the second pass. In crude methods of numerically computing
deviation and means, one can compute the same standard deviation in one pass. Equation \ref{eqn_stddev} shows
this method by accumulating the sums of $x_i$ and $x_{i}^{2}$. This subtraction can result in 
loss of accuracy if the square of the mean is large while the variance is small.

\begin{equation} \label{eqn_stddev}
  \begin{split}
    \sigma =\sqrt{(n\sum_{1\leq i\leq n}x_{i}^{2}-(\sum_{1\leq i\leq n} x_{i})^2)/n(n-1)}
  \end{split}
\end{equation}

Welford's method simply keeps a running sum of the data, number of samples, 
and deviation from data collected so far, and the user can view the sample variance and
mean at anytime during the computational process to view their progression. Equation \ref{eqn_w1} show
the recurrence formula from Welford which takes into account only the current and previous sample values.
Equation \ref{eqn_w2} shows the computation of the mean, and Equation \ref{eqn_w3} for the standard deviation. 

\begin{equation} \label{eqn_w1}
  \begin{split}
    M_{2,n}=M_{2,n-1}+(x_n -\bar{x}_{n-1})(x_n -\bar{x}_{n})
  \end{split}
\end{equation}

\begin{equation} \label{eqn_w2}
  \begin{split}
    \sigma_{n}^{2}=\frac{M_{2,n}}{n}
  \end{split}
\end{equation}

\begin{equation} \label{eqn_w3}
  \begin{split}
    s_{n}^{2}=\frac{M_{2,n}}{n-1}
  \end{split}
\end{equation}






\subsection{Imaging}


\subsection{Spectroscopy}



\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{sample-figure}
  \caption{Sample figure}
  \label{fig:sample}
\end{figure}




\section{Evaluation}

 
\subsection{Statistical Analysis}


\subsection{Imaging}


\subsection{Spectroscopy}


\section{Results}





\section{Applications}




\section{Conclusion}





\bibliographystyle{acm}
\bibliography{sigproc} 

\end{document}
